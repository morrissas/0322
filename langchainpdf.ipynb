{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import config \n",
    "\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = config.PineconeKey\n",
    "Pineconekey = config.PineconeKey\n",
    "OpenAIKey = config.OpenAIKey\n",
    "IndexName = config.IndexName\n",
    "Localize = config.localize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"docs/230811236.pdf\"   # load pdf text\n",
    "loader = PyPDFLoader(file_path)  \n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{21}\n",
      "[Document(page_content='1\\nROSGPT Vision: Commanding Robots Using Only\\nLanguage Models’ Prompts\\nBilel Benjdira, Anis Koubaa, Anas M. Ali\\nRobotics and Internet of Things Lab (RIOTU Lab), Prince Sultan University, Saudi Arabia\\nbbenjdira@psu.edu.sa; akoubaa@psu.edu.sa; aaboessa@psu.edu.sa\\nAbstract —In this paper, we argue that the next generation\\nof robots can be commanded using only Language Models’\\nprompts. Every prompt interrogates separately a specific Robotic\\nModality via its Modality Language Model (MLM). A central\\nTask Modality mediates the whole communication to execute the\\nrobotic mission via a Large Language Model (LLM). This paper\\ngives this new robotic design pattern the name of: Prompting\\nRobotic Modalities (PRM). Moreover, this paper applies this\\nPRM design pattern in building a new robotic framework named\\nROSGPT Vision. ROSGPT Vision allows the execution of a\\nrobotic task using only two prompts: a Visual and an LLM\\nprompt. The Visual Prompt extracts, in natural language, the vi-\\nsual semantic features related to the task under consideration (Vi-\\nsual Robotic Modality). Meanwhile, the LLM Prompt regulates\\nthe robotic reaction to the visual description (Task Modality).\\nThe framework automates all the mechanisms behind these two\\nprompts. The framework enables the robot to address complex\\nreal-world scenarios by processing visual data, making informed\\ndecisions, and carrying out actions automatically. The framework\\ncomprises one generic vision module and two independent ROS\\nnodes. As a test application, we used ROSGPT Vision to develop\\nCarMate, which monitors the driver’s distraction on the roads\\nand makes real-time vocal notifications to the driver. We showed\\nhow ROSGPT Vision significantly reduced the development cost\\ncompared to traditional methods. We demonstrated how to im-\\nprove the quality of the application by optimizing the prompting\\nstrategies, without delving into technical details. ROSGPT Vision\\nis shared with the community1to advance robotic research in this\\ndirection and to build more robotic frameworks that implement\\nthe PRM design pattern and enables controlling robots using\\nonly prompts.\\nIndex Terms —Robotic Design Patterns, Robotic Modalities,\\nPrompting Robotic Modalities, Large Language Models, LLMs,\\nVision Language Models, VLMs, Robotic Operating System,\\nROS, ROS2, Robotic Prompt Engineering, Visual prompt, LLM\\nprompt\\nI. I NTRODUCTION\\nRecent advances in Large Language Models (LLMs),\\nRobotics, and Computer Vision have developed new research\\ntrends to bridge the gap between visual and sensor data\\ninterpretation, language modeling, and robotic actions. This\\nresearch is inspired by ROSGPT [1], which built the first\\nframework to convert human language into explicit robotic\\ncommands using the ChatGPT LLM, helping to interact with\\nrobotics in natural language. However, ROSGPT [1], did not\\nconsider multimode interaction patterns and thus limited to\\nthe textual modality. The current study enlarged the scope by\\n1link: https://github.com/bilel-bj/ROSGPT Visionsuggesting a new robotic design pattern and a new robotic\\nframework. We proposed the Prompting Robotic Modalities\\n(PRM) design pattern to conceive a common ground for\\nfuture developments in this field. Also, we proposed the\\nROSGPT Vision framework based on PRM to make seamless\\nrobotic interaction with visual data by the intermediate of\\nLanguage Models. In the following subsections, a closer look\\nhas been made at the state of the art of Language Models,\\nrobotics, and visual data interpretation. This helps to deduce\\nthe current gaps in the literature targeted by this work.\\nA. Large Language Models (LLMs) and their impact on\\nRobotics\\nIn recent years, the development of large language models\\n(LLMs) has revolutionized the field of natural language pro-\\ncessing (NLP) with impressive capabilities, such as generating\\nhuman-like text, understanding context, answering questions,\\nand even translating languages [2, 3]. LLMs has brought about\\nsignificant advancements with a wide range of applications\\nspanning multiple domains [4]. Notable examples include\\nOpenAI’s GPT-3 [2] and ChatGPT [5], which have demon-\\nstrated remarkable language processing and understanding ca-\\npabilities. These developments have created new opportunities\\nfor enhancing human-robot interaction and communication, as\\nwell as other aspects of robotics.\\nThe research community has shown a growing interest in\\nharnessing the potential of LLMs to improve various aspects\\nof robotics. For instance, LLMs have been used to facilitate\\nnatural language understanding in robots, enabling them to\\nprocess and respond to human commands more effectively [6].\\nAdditionally, LLMs have been employed in human-robot col-\\nlaboration, where robots can generate context-aware responses\\nto human instructions, promoting seamless cooperation [7].\\nLLMs have been proven to be highly useful in several\\napplications due to their remarkable ability to learn new\\ncommunication patterns with either zero-shot orfew-shot\\nlearning . In zero-shot learning, the LLM can generate accurate\\nresponses for tasks it has never been trained on, while in few-\\nshot learning, it can effectively adapt to new tasks with only a\\nfew training examples. This adaptability is a key advantage of\\nLLMs, allowing them to learn quickly and improve in various\\ncontexts.\\nThe exceptional adaptability of LLMs stems from prompt\\nengineering techniques, which equip them to tackle intricate\\nnatural language processing and comprehension tasks. ByarXiv:2308.11236v2  [cs.RO]  23 Aug 2023', metadata={'source': 'docs/230811236.pdf', 'page': 0}), Document(page_content='2\\nsupplying LLMs with precise prompts or instructions, they can\\nproduce highly accurate and pertinent responses to input text.\\nThis versatility has rendered LLMs invaluable across diverse\\napplications, encompassing language translation, text summa-\\nrization, chatbots, and human-robot interaction. Moreover, the\\npotential of LLMs in the area of explainable AI (XAI) for\\nrobotics has been explored, with LLMs generating human-\\nreadable explanations for robotic decision-making processes,\\nenhancing trust and understanding between humans and ma-\\nchines [8]. LLMs have also been employed to improve robotic\\nlearning from demonstrations, where they generate natural\\nlanguage descriptions of expert demonstrations to teach robots\\nnew tasks through imitation learning [9].\\nAs the capabilities of LLMs continue to advance, it is\\nanticipated that their integration with robotic systems will\\nlead to increasingly sophisticated and capable machines that\\ncan interact seamlessly with humans and their environment.\\nThe exploration of LLMs in robotics is still in its early\\nstages, and it is expected that future research will uncover\\nnew applications and techniques for leveraging the power of\\nLLMs in the field of robotics.\\nLarge language models (LLMs) possess remarkable capa-\\nbilities to comprehend human language inputs and generate\\ncontextually relevant responses across diverse applications.\\nTheir inherent versatility allows them to be fine-tuned for\\nspecialized tasks, such as language translation or text sum-\\nmarization. Renowned examples of LLMs include OpenAI’s\\nGPT-3 [2] and GPT-4 [10, 11], Google’s BERT [12], and T5\\n[13]. These models exhibit exceptional abilities that set them\\napart from smaller pre-trained language models (PLMs).\\nDespite the remarkable performance demonstrated by LLMs\\nin tackling intricate tasks, their underlying capabilities remain\\nan active area of research. The scientific community continues\\nto explore and investigate the full extent of LLMs’ intrinsic\\nproperties to gain a deeper understanding of their potential [4].\\nA key challenge in robotics is developing systems that\\ncan effectively communicate and collaborate with humans.\\nTo address this challenge, researchers have begun to integrate\\nLLMs into robotic systems to facilitate natural language under-\\nstanding and generation. For instance, [6] demonstrated the use\\nof LLMs to enable a robot to understand and execute natural\\nlanguage commands for tasks such as object manipulation\\nand navigation. Similarly, [7] proposed a method to combine\\nLLMs with robotics to improve human-robot collaboration by\\nallowing robots to generate context-aware responses to human\\ninstructions.\\nFurthermore, LLMs have been used to improve robotic\\nlearning from demonstrations. In [9], the authors employed\\nLLMs to generate natural language descriptions of expert\\ndemonstrations, which were then used to teach robots new\\ntasks through imitation learning. This approach enables robots\\nto acquire new skills more efficiently by leveraging the vast\\nknowledge encoded in LLMs.\\nIn summary, the impact of LLMs on robotics is multi-\\nfaceted, with potential applications in human-robot communi-\\ncation, explainable AI, and robotic learning from demonstra-\\ntions. As LLMs continue to advance, it is expected that their\\nintegration with robotic systems will lead to increasingly so-phisticated and capable machines that can seamlessly interact\\nwith humans and their environment.\\nB. Image Understanding for Robotic Manipulation\\nImage understanding plays a crucial role in advancing the\\nfield of robotic manipulation, as it allows robots to perceive\\nand interact with their surrounding environment more effec-\\ntively [14, 15]. The integration of computer vision and machine\\nlearning techniques has greatly improved the performance of\\nrobotic systems, enabling them to autonomously recognize,\\ngrasp, and manipulate objects in various scenarios [16, 17].\\nDeep learning, in particular, has been instrumental in driv-\\ning progress in this domain [18, 19]. Convolutional Neural\\nNetworks (CNNs) and other advanced neural network archi-\\ntectures have demonstrated remarkable success in learning to\\ndetect and localize objects, predict grasps, and plan manip-\\nulations from raw image data [20, 21]. These advances have\\nfacilitated more natural and efficient human-robot interaction,\\nas well as enabled robots to handle complex tasks in diverse\\nenvironments [22, 23].\\nDespite the significant progress made in recent years, nu-\\nmerous challenges still exist in the field of image understand-\\ning for robotic manipulation. These include the need for robust\\nobject recognition under varying conditions, generalization\\nacross different object categories and environments, and real-\\ntime processing for fast and adaptive robotic control [24, 25].\\nAddressing these challenges will pave the way for more\\nsophisticated robotic systems capable of performing a wide\\nrange of tasks autonomously and in cooperation with humans.\\nC. Recent Advances in Vision-Language Models\\nVision-language models (VLMs) are a type of artificial\\nintelligence (AI) that can comprehend and generate both visual\\nand textual information [26, 27]. Trained on enormous datasets\\nof images and text, they have been utilized for a range of\\ntasks such as image captioning, visual question answering,\\nand visual reasoning [28–31].\\nWith their potential to transform the way we interact with\\ncomputers, there has been an upsurge of interest in VLMs\\nin recent years [32]. VLMs can create more intuitive and\\nengaging user interfaces and automate tasks that are currently\\nperformed by humans, such as image captioning and visual\\nquestion answering.\\nSome of the notable advances in VLMs include:\\n•Larger and more complex models: VLMs have grown\\nincreasingly large and complex, due to the availability\\nof larger and more diverse datasets and the development\\nof more powerful LLMs [2]. For instance, the GPT-\\n3 language model has 175 billion parameters, while\\nthe ViLBERT vision-language model boasts 340 million\\nparameters [26].\\n•Multimodal learning: The capability of VLMs to learn\\nfrom both visual and textual data has been realized using\\nmultimodal neural networks that can merge information\\nfrom both modalities [29].\\n•Transfer learning: Knowledge gained from Efficient\\nVision encoders is used to enhance the performance of', metadata={'source': 'docs/230811236.pdf', 'page': 1}), Document(page_content='3\\nthe VLM models, which can be considered as a form\\nof multimodal transfer learning [29]. For instance, the\\nMiniGPT-4 model [33] used the efficient Q-former [34]\\narchitecture as an image feature extractor in their bi-\\nmodal network.\\nThese advances have contributed to significant improve-\\nments in the performance of VLMs on various tasks. For ex-\\nample, the MiniGPT-4 [33] and LLava [35] have demonstrated\\nsatisfying capabilities in reasoning about image data in natural\\nhuman language.\\nThe future of VLMs appears very promising. As they\\ncontinue to grow in size and complexity, VLMs will be capable\\nof performing increasingly complex tasks. This progress is\\nexpected to facilitate the development of innovative applica-\\ntions such as virtual assistants that understand and respond in\\nnatural language and robots that can perceive and understand\\ntheir environment [29].\\nVLMs, however, do face a few challenges:\\n•Data scarcity: One major challenge for VLMs is the lack\\nof data. For effective learning from both visual and textual\\ndata, VLMs require large and diverse datasets, which are\\noften costly and time-consuming to collect [36].\\n•Bias: Since VLMs are trained on data created by humans,\\nthey can reflect human bias in their outputs, leading to\\ninaccurate or unfair results [37].\\n•Interpretability: Due to their complex training datasets\\nand multifactorial decision-making processes, VLMs as\\nwell as many other deep learning models are often hard\\nto interpret [38]. This can make it difficult to understand\\nwhy a VLM makes a specific prediction or decision.\\nDespite these challenges, VLMs are a promising new tech-\\nnology with the potential to revolutionize our interaction with\\ncomputers. As improvements continue, VLMs are expected to\\nperform an increasingly complex array of tasks, leading to the\\ndevelopment of new and innovative applications.\\nD. Novelty of IRM and ROSGPT Vision\\nAs shown above, the robotic community needs to profit\\nfrom the development of LLMs and VLMs to design new\\nmethods that facilitate the execution of robotic tasks. This\\npaper introduces a new Robotic Design Pattern: the PRM\\n(Prompting Robotic Modalities). It presents a novel approach\\nto robotic design, emphasizing the distinct querying of individ-\\nual sensory and interaction channels. This innovative pattern\\nsuggests that future robots should be equipped to address\\neach modality, such as vision or audition, through specific\\n”prompts.” These prompts, tailored commands or inquiries,\\nallow for precise data gathering from each sensory input,\\nensuring that every modality operates as an independent data\\nsource. Every Modality should be associated with its own\\nModality Language Model (MLM). The MLM helps to inquire\\nthe data in a promptable manner. Form example, Vision-\\nLanguage Model (VLM) are the MLM specific the Visual\\nModality. It used to be used to prompt visual data using\\ntextual input. However, to synthesize this data for cohesive\\nrobot action, the central ”Task Modality” acts as a mediator.\\nIt collates information from each prompt, guiding the robot’soverarching decisions and actions. The robots actively probe\\neach modality rather than passively receiving data. the Task\\nModality is connected to aLarge Language Model that should\\nhave advanced eliciting capability. In essence, the PRM design\\nopt for a modular approach, offering robots greater flexibility\\nand precision. By treating each sensory channel as a separate\\nmodule and having a central system for decision-making,\\nrobots can adapt better to their environment, paving the way\\nfor more responsive and adaptable robotic systems in the\\nfuture.\\nTo prove the validity of this new design pattern, we apply\\nit for the task that needs only Vision Modality such as tasks\\nthat need to take informative decision about visual content\\ndetected by the Robotic camera. We applied the PRM Design\\npattern to develop the ROSGPT Vision framework. The input\\nimage given to the robot is converted into natural textual\\nlanguage with enhanced reasoning and eliciting features by\\nthe intermediate of the interrogating visual prompt. By doing\\nso, we aim to bridge the gap between visual perception and\\nnatural language understanding in the context of human-robot\\ninteraction. This approach enables robots not only to process\\nand analyze visual information but also to understand and\\nact upon human language-based commands and instructions.\\nFurthermore, the incorporation of enhanced reasoning and\\neliciting features in the generated textual language allows\\nrobots to interpret complex instructions better, ask relevant\\nquestions for clarification, and provide more informative feed-\\nback to humans. Ultimately, this method has the potential to\\nfacilitate more intuitive, efficient, and effective communication\\nbetween humans and robots, leading to improved collabo-\\nration and performance in various tasks and environments.\\nIn the rapidly evolving field of robotics, the integration of\\nlanguage models with computer vision capabilities has become\\na significant area of interest. This paper explain the PRM\\ndesign pattern and introduces a new robotics framework that\\nhelps the robots interact with images in natural language: the\\nROSGPT Vision. This framework provides an intersection of\\nlanguage understanding and image perception to extend the\\ncapabilities of robots, enabling them not only to perceive their\\nenvironment visually but also to interpret and interact with\\nit in a meaningful way. ROSGPT Vision is an introductory\\nstep forward in the quest for sophisticated, autonomous, and\\nintelligent robotic systems. The utility of the PRM pattern\\nand the ROSGPT Vision framework is vast, with potential\\napplications ranging from domestic helper robots to advanced\\nmanufacturing systems, autonomous vehicles, and more.\\nThe name ROSGPT Vision is given to reflect the inherent\\nintegration of LLMs (Mostly based on GPT architecture) with\\nRobot Operating System ROS, and Vision Language Models\\n(VLMs). In the context of this study, we use the abbreviation\\nROS to refer to ROS 1 and ROS 2. With ROSGPT Vision, the\\nrobot can translate image data into natural human language\\ncommands using task-designed visual prompts. The textual\\noutput will be converted into LLMs to acquire more reasoning\\ncapabilities to deduce the best action to take following the\\ncontext of the robot task. Both this work and ROSGPT [1]\\ntarget to improve the state of the art in the robotic field\\nby making seamless interpretation of sensor data in natural', metadata={'source': 'docs/230811236.pdf', 'page': 2}), Document(page_content='4\\nhuman language. ROSGPT [1] makes a broker that converts\\nhuman commands given in natural language to explicit robotic\\ncommands, while ROSGPT Vision makes a broker to convert\\nimage data into natural human language to benefit from the\\npower of LLMs and to convert it into more customizable robot\\ncommands.\\nThe most near research works to our study are [39] and [40].\\nIn [39], the authors used ChatGPT to enable natural language-\\nbased control of many robotic platforms such as drones,\\nrobot arms, and home assistant robots. They made design\\nprinciples to help language models solve robotic tasks. They\\nintroduced an AirSim simulation environment integrated with\\nChatGPT. They also developed PromptCraft, a collaborative\\nopen-source platform that collects some prompting strategies\\nfor different robotics tasks. In [40], the author introduced\\nrobotGPT, by presenting ChatGPT’s principles and discussing\\nhow to enhance robotic intelligence through the medium of\\nChatGPT.\\nDistinct from the approaches presented above, ROS-\\nGPT Vision introduces various innovative elements that sig-\\nnificantly advance the field of robot interaction with visual\\ndata. Firstly, ROSGPT Vision divides the robot interaction\\nwith visual data into many modules explained in the next\\nsection. Second, ROSGPT Vision makes a distinction between\\nthe extraction of image semantics and the robot ontology\\nassociated with one given task. Third, it completely separates\\nthe prompting logic from the whole architecture, so that the\\nend user will focus mostly on developing the most accurate\\nprompts for his task in the YAML files associated with it.\\nFinally, the shared ROS package is designed following a clear\\nand extensible architecture that could be used and understood\\nby normal users. PRM and ROSGPT Vision tries to bridge\\nthe gap between Robotics, Natural Language Processing, and\\nComputer Vision by orienting the next Robotic developers\\ntowards focusing on designing the best prompts for their tasks\\nand to explicitly improving the clarity of the robot ontology\\nbehind these prompts. This work represents a stepping stone\\nfor the ROS, NLP, and Computer Vision communities to\\ncollaboratively advance this interdisciplinary research field.\\nII. PROMPTING ROBOTIC MODALITIES (PRM) D ESIGN\\nPATTERN\\nThe concept of ”Prompting Robotic Modalities (PRM)”\\nintroduces a novel approach to the domain of robotic design,\\nemphasizing modularity and targeted querying of individual\\nsensory and interaction channels. The global idea is show in\\nFigure 1. At the heart of this design pattern is the idea that\\nrobots, should be constructed with the capability to prompt\\neach of their sensory or interaction modalities—be it vision,\\naudition, or tactile sensing—separately and distinctly. This\\nis done by incorporating every robotic modality inside a\\nspecific Modality Language Model (MLM). The MLM will\\nbe trained to interrogating the Modality data in textual manner\\ngiving more insights on it. As an example, the Visual Robotic\\nModality is incorporated in ROSGPT Vision (see next section)\\ninside a VLM (Vision Language Model). The VLM helps\\nto enquire the images captured by the robotic camera using\\ntextual inputs.\\nFig. 1: The robotic design pattern ”Prompting Robotic Modal-\\nities (PRM)”\\nThe ”prompt” in this context can be understood as a specific\\ncommand or inquiry aimed at a particular modality. For\\ninstance, when the robot needs to gather visual data about\\nits surroundings, a distinct prompt might be issued to its\\nvision modality, asking it to identify objects within a certain\\nrange. Similarly, if the robot needs to ascertain ambient noise\\nlevels, a separate prompt might be directed at its auditory\\nmodality. The beauty of this approach lies in its precision;\\nby addressing each modality with individualized prompts, the\\nsystem ensures that every sensory input or output is treated as a\\nunique and independent source of data, devoid of unnecessary\\ncross-modality interference.\\nHowever, while the idea of querying each modality sep-\\narately offers a high degree of precision, it also presents a\\nchallenge: how does one ensure that the data from these var-\\nious modalities is synthesized effectively to guide the robot’s\\noverarching actions and decisions? This is where the concept\\nof the ”Task Modality” comes into play. Acting as the central\\nsystem of the robot, the Task Modality serves as a mediator or\\ncoordinator. Its primary role is to communicate with each of\\nthe robot’s modalities, collate the information obtained from\\nthe distinct prompts, and then make informed decisions on the\\nbest course of action to ensure the successful execution of the\\nrobot’s mission. Different from normal Robotic Modalities,\\nThe Task Modality is connected to a generic LLM not to an\\nMLM.\\nInstead of the robot passively receiving data from its various\\nsensors, the PRM approach necessitates an active querying or\\nprobing of each modality. Such an active approach to data\\ngathering might very well lead to the creation of robotic\\nsystems that are not only more responsive but also more\\nadept at handling real-time challenges and changes in their\\nenvironment.\\nIn summary, the PRM design pattern is a testament to the\\npotential benefits of a modular approach in robotic design. By\\ntreating each sensory or interaction channel as an independent\\npromptable module, robots can achieve a higher degree of flex-\\nibility and precision in their operations. Moreover, the central\\nTask Modality ensures that while each modality operates inde-\\npendently, there remains a cohesive system in place to guide\\nthe robot’s overall behavior. Such a design could pave the', metadata={'source': 'docs/230811236.pdf', 'page': 3}), Document(page_content='5\\nway for the development of robots that can easily adapt to new\\ntasks, integrate new modalities, and respond more efficiently to\\nthe demands of their environment. This paper applies the PRM\\ndesign pattern to conceive the ROSGPT Vision framework,\\ndetailed in the next section.\\nIII. C ONCEPTUAL ARCHITECTURE OF ROSGPT VISION\\nThe ROSGPT Vision architecture is described in Figure 2.\\nVisual data is captured by the camera, transferred to the Image\\nSemantics module to generate an accurate description guided\\nby the Visual prompt given by the user. The Camera node\\nalways updates the Image Description topic with the given\\ndescription of the image. This description is transferred to\\nthe GPT Consultation node, which interacts with the LLM\\nto generate accurate decisions and instructions that should\\nbe made by the robot. The interaction with the LLM is also\\nguided by an LLM prompt given by the user to optimize the\\nrobotic job. The prompting logic is separated from the whole\\narchitecture in order to let the end user focus on optimizing\\nthis critical part of the Robotic design. The framework is\\nbased on the PRM design pattern, where the GPT Consultation\\nNode refers the Task Modality controlled via the LLM prompt.\\nThe Camera Node is the embodiment of the Visual Robotic\\nModality where the Visual Language Model (VLM) associated\\nto this specific modality is included in the Image Semantics\\nModule. We can see that the Camera Node helps to inspect the\\nimages captured from the robotic camera via a specific prompt\\n(the Visual Prompt). In the next subsection, more details are\\ngiven about every component of the framework.\\nA.Image Semantics: Extracting meaning from Images\\nThe Image Semantic module is the primary building block\\nin ROSGPT Vision. It serves as a bridge between visual\\ndata and human language. This module’s primary function\\nis to extract and interpret meaning from images in a way\\nthat mirrors natural human language understanding. It uses\\ndifferent deep learning models to analyze the contents of an\\nimage in various ways. It acts by identifying objects, patterns,\\nand relationships within the image, and then translates these\\nvisual elements into semantic descriptions in natural language.\\nMoreover, it can use direct image captioning models, visual\\nchatting models to achieve this goal. The output of any method\\ncontained in this module is to generate a descriptive narration\\nfor an image in a human-natural manner. This output will be\\nguided for a specific robotic task by customized prompts.\\nIt assembles many predefined methods for this topic, such\\nas LLA V A model [35], MiniGPT-4 [33], and SAM (Segment\\nAnything) [41]:\\n1)LLaVA Model\\nThe LLaV A (Large Language-and-Vision Assistant) model\\nrepresents a novel end-to-end trained large multimodal model\\nthat combines a vision encoder and Vicuna for general-purpose\\nvisual and language understanding [35]. LLaV A connects a\\npre-trained CLIP ViT-L/14 visual encoder and a large language\\nmodel, Vicuna, using a simple projection matrix. It employs\\na two-stage instruction-tuning procedure:1) Pre-training for Feature Alignment: In this stage, only\\nthe projection matrix is updated, based on a subset of\\nCC3M.\\n2) Fine-tuning End-to-End: In this stage, both the projec-\\ntion matrix and the large language model are updated.\\nTwo different use scenarios are considered:\\n•Visual Chat: LLaV A is fine-tuned on generated mul-\\ntimodal instruction-following data for daily user-\\noriented applications.\\n•Science QA: LLaV A is fine-tuned on a multimodal\\nreasoning dataset for the science domain.\\nLLaV A has demonstrated impressive multimodal chat abilities\\nand achieved a new state-of-the-art accuracy of 92.53% when\\nfine-tuned on Science QA, thus setting a benchmark in the\\ndomain of visual and language understanding [35].\\n2)MiniGPT-4 Model\\nThe MiniGPT-4 [33] model, similarly to LLA V A, is an\\nadvancement in the field of vision-language understanding that\\nleverages an advanced large language model (LLM), Vicuna,\\nand demonstrates multi-modal abilities similar to the more\\ncomplex GPT-4. MiniGPT-4 consists of a vision encoder with\\na pre-trained ViT and Q-Former, a single linear projection\\nlayer, and an advanced Vicuna large language model. The\\nmodel only requires training the linear layer to align the\\nvisual features with the Vicuna [33]. MiniGPT-4 exhibits a\\nwide range of capabilities such as detailed image description\\ngeneration, website creation from hand-written drafts, writing\\nstories and poems inspired by given images, providing solu-\\ntions to problems shown in images, and teaching users how to\\ncook based on food photos. It is also highly computationally\\nefficient, training a projection layer utilizing approximately 5\\nmillion aligned image-text pairs [33].\\n3)The Segment Anything Model (SAM) model\\nThe Segment Anything Model (SAM) [41] is a founda-\\ntional model for image segmentation that is designed to be\\npromptable, meaning it can be guided to perform specific\\ntasks via prompts. The task of SAM is to return a valid\\nsegmentation mask given any segmentation prompt. A prompt\\ncan include spatial or text information identifying an object,\\nand the output should be a reasonable mask for at least one\\nof the objects specified by the prompt. SAM consists of three\\nmain components: an image encoder that computes an image\\nembedding, a prompt encoder that embeds prompts, and a\\nlightweight mask decoder that predicts segmentation masks\\nbased on the image and prompt embeddings. To train SAM,\\na diverse, large-scale source of data is needed. The authors\\nbuilt a ”data engine” to collect a dataset of over 1 billion\\nmasks. This engine has three stages: assisted-manual, semi-\\nautomatic, and fully automatic. In the first stage, SAM assists\\nannotators in annotating masks. In the second stage, SAM\\ncan automatically generate masks for a subset of objects, and\\nannotators focus on annotating the remaining objects. In the\\nfinal stage, SAM is prompted with a regular grid of foreground\\npoints, yielding an average of around 100 high-quality masks\\nper image. The final dataset, SA-1B, includes more than 1\\nbillion masks from 11 million licensed and privacy-preserving\\nimages.', metadata={'source': 'docs/230811236.pdf', 'page': 4}), Document(page_content='6\\nROS (Robotic Operating System) \\nRobot Hardware \\nCamera sensor ROSGPT_Vision \\nImage Semantics \\n \\nCaptions  \\n(Normal, \\nDense…) Visual  \\nQuestion  \\nAnswering:  \\n(BLIP, Coca,...) Visual  \\nDialog:  \\n(LLaVa, \\nLLaMa- \\nAdapter, \\nMini-GPT4…) \\nX-Anything  \\n(SAM, SEEM…) Image \\nDescription \\nT opic LLM \\nAPIs Camera \\nNode \\nGPT \\nConsultation \\nNode GPT \\nConsultation \\nT opic Visual Prompt LLM Prompt \\nFig. 2: ROSGPT Vision Architecture for Natural Language Understanding of Visual data\\nB.The Camera Node and Image Description Topic\\nAs represented in Figure 2, ROSGPT Vision contains two\\nROS nodes: Camera Node and GPT Consultation Node.\\nThey interact using two ROS topics: Image Description and\\nGPT Consultation. The Camera Node utilizes the Image\\nSemantics module to extract detailed descriptions from an\\ninput image. This node is designed to handle real-time image\\nstreaming, processing each image frame as it comes in. It gets\\nthe description from the Image Semantics module, and then\\npublishes it to a ROS topic named Image Description. The\\nGPT Consultation Node is subscribed to this topic, allowing\\nit to receive and utilize these image descriptions in real-time.\\nThe Camera Node class converts the input image into\\ndescriptive representations, following a predetermined prompt\\naccording to Algorithm 1. It can use both video paths and we-\\nbcam streams. Subsequently, the Camera Node class proceeds\\nto transmit the resulting descriptions to the GPT Consultation\\nclass via the Image Description topic. Figure 3 shows the cam-\\neranode and GPT Consultation classes diagrams following\\nthe the Unified Modeling Language (UML) framework.\\nC.The GPT Consultation Node and Topic\\nThe two nodes (Camera and GPT Consultation) are com-\\npletely separated. The task logic must be completely separated\\nfrom the image description logic, following the PRM design\\npattern. The Camera node focuses on accurately describing the\\nimage without being concerned about the actions that should\\nbe taken based on it. The second node, the GPT Consul-\\ntation node, is an Ontology-Based Action Planner Node. It\\nsubscribes to the Image Description topic published by the\\nCamera node and uses this information to suggest appropriate\\nactions for the robot. The node should explicitly emphasizeAlgorithm 1 Camera node\\nInputs :\\n1)DM : The description model [image2text] [string]\\n2)WC: The webcam used [True or False]\\n3)V P: The video path, add path if WC is False [string]\\n4)f: The frame interval [integer]\\nprocedure\\nfcounter←0\\nifWC =True then\\nwhile WC =True do\\niffcounter modf= 0 then\\nframe←camera.get image()\\ntext←DM(frame )\\nsend2publisher (text, ’Image description’ )\\nfcounter←fcounter + 1\\nend if\\nend while\\nelse\\nwhile video.is opened() do\\niffcounter modf= 0 then\\nframe←video.get image()\\ntext←DM(frame )\\nsend2publisher (text, ’Image description’ )\\nfcounter←fcounter + 1\\nend if\\nend while\\nend if\\nconsultation ←subscribe (’GPT consultation’ )\\nend procedure', metadata={'source': 'docs/230811236.pdf', 'page': 5}), Document(page_content='7\\nFig. 3: UML diagram\\na specially designed robot ontology behind the given task. A\\nrobot ontology is a formal representation of knowledge within\\nthe robotics domain. This ontology includes definitions of\\nconcepts, properties, and relationships pertaining to the robot\\nand its environment, providing a structured and semantically\\nrich framework for reasoning and decision-making.\\nThe GPT Consultation Node uses the robot ontology to\\nmap the image descriptions to potential actions according\\nto Algorithm 2. The final feedback relevant to the task is\\npublished into the topic GPT Consultation. This name is\\nchosen over the name GPT Action because it is more generic.\\nConsultation refers to the real feedback from GPT, which can\\nbe just a general notification, a decision-aid estimation, or an\\naction to be taken by other nodes.\\nAlgorithm 2 GPT Consultation node\\nInputs :\\n1)Key : The OpenAI key [string]\\n2)Temp : The ChatGPT temperature [float]\\nprocedure\\ntext←subscribe (’Image description’ )\\nresponse ←askGPT (Key, Temp, text )\\nsend2publisher (response, ’GPT consultation’ )\\nend procedure\\nThe main component of the node is its interaction with the\\nLLM, because it represents the Task Modality in the PRM\\ndesign pattern. The framework contains several parameters\\ncollected in one YAML file. YAML file consists of the Task\\nname and parameters of Camera node and GPT node andimage description models. The main two parameters to change\\nin the YAML file are the prompts: the LLM and the Visual\\nPrompts. Other parameters can be let unchanged.\\nD.Visual and LLM prompts\\nROSGPT Vision is configured using a single YAML (Yet\\nAnother Markup Language) file which describes the robot\\ntask and saved under the folder cfg of the ROSGPT Vision\\nfolder. YAML files are commonly used in ROS for their\\nhuman-readable data serialization format, allowing for easy\\nconfiguration and parameter setting. Notably, this file in-\\ncludes the configurations for both visual and language model\\n(LLM) prompts. Visual prompts guide the image processing\\nand semantic extraction capabilities of the node, while LLM\\nprompts are used to instruct the language model to generate\\nappropriate textual descriptions or actions based on the image\\nsemantics. This consolidated configuration approach allows\\nfor efficient setup and modification of the ROSGPT Vision\\nnode, facilitating its integration and use in various robotic\\napplications.\\nFig. 4: YAML file format in ROSGPT Vision\\nThe structure of the YAML file is provided in the the Figure\\n4. The YAML contains 6 main sections of configurations\\nparameters:\\n•Task_name : This field specifies the name of the task\\nthat the ROS system is configured to perform.\\n•ROSGPT_Vision_Camera_Node : This\\nsection contains the configuration for the\\nROSGPT Vision Camera Node.', metadata={'source': 'docs/230811236.pdf', 'page': 6}), Document(page_content='8\\n–Image_Description_Method : This field spec-\\nifies the method used by the node to generate de-\\nscriptions from images. It can be one of the currently\\ndeveloped methods: MiniGPT4, LLaV A, or SAM.\\nThe configurations needed for everyone of them is\\nput separately at the end of this file.\\n–Vision_prompt : This field specifies the prompt\\nused to guide the image description process.\\n–Output_video : This field specifies the path or the\\nname of where to save the output video file.\\n•GPT_Consultation_Node : This section contains the\\nconfiguration for the GPT Consultation Node.\\n–llm_prompt : This field specifies the prompt used\\nto guide the language model.\\n–GPT_temperature : This field specifies the tem-\\nperature parameter for the GPT model, which con-\\ntrols the randomness of the model’s output.\\n•MiniGPT4_parameters : This section contains the\\nconfiguration for the MiniGPT4 model. It should be\\nclearly set if the model is used in this task, otherwise\\nit could be empty.\\n–configuration : This field specifies the path for\\nthe configuration file of MiniGPT4.\\n–temperature_miniGPT4 : This field specifies\\nthe temperature parameter for the MiniGPT4 model.\\n•llava_parameters : This section contains the config-\\nuration for the llavA model (if used).\\n–temperature_llavA : This field specifies the\\ntemperature parameter for the llavA model.\\n•SAM_parameters : This section contains the configu-\\nration for the SAM model.\\n–weights_SAM : This field specifies the weights\\nused by the SAM model.\\nE. ROSGPT Vision Implementation on ROS2\\nROSGPT Vision code is shared with the community, under\\nthe link: https://github.com/bilel-bj/ROSGPT Vision. Our aim\\nis to fosters further advancements in the realm of visual data\\ncomprehension and interaction, leveraging the capabilities of\\nROS in conjunction with the potential of Language and Vision\\nLanguage Models (LLMs and VLMs).\\nIV. P ROOF -OF-CONCEPT :THE CARMATEAPPLICATION\\nWe present a software application named CarMate based on\\nthe ROSGPT Vision framework, and the PRM design pattern.\\nCarMate is an advanced driver monitoring and assistance\\napplication designed to ensure safe and efficient driving expe-\\nriences. CarMate is developed by just configuring the YAML\\nfile of the ROSGPT Vision framework, and mainly by just\\nsetting the two prompts. Through a camera mounted inside\\nthe car, CarMate continuously monitors the driver’s actions,\\nexpressions, and attention levels. The system is capable of\\ndetecting unsafe behaviors such as distracted driving, drowsi-\\nness, or aggressive driving patterns. When such behaviors are\\nidentified, CarMate issues timely alerts heard by the driver,\\nprompting corrective action.In addition to real-time monitoring and alerts, CarMate\\nhelps generate personalized driving reports. These reports\\nsummarize the driver’s behavior over a particular period, high-\\nlighting areas of strength and those requiring improvement.\\nBy providing clear, actionable insights, CarMate aids drivers\\nin developing safer driving habits.\\nA.The Architecture of CarMate\\nThe CarMate application’s architecture follows the ROS-\\nGPT Vision framework, as depicted in Figure 5. Initially, the\\napp captures driver images at five-second intervals. Subse-\\nquently, the image semantics model (LLA V A) processes the\\nimages by employing a conditional vision description prompt,\\nas indicated in Figure 4. The prompt used is [”Describe the\\ndriver’s current level of focus on driving based on the visual\\ncues. Answer with one short sentence.”]. Following this, the\\nCamera node publishes the vision description to the image\\ndescription topic. The GPT consultation node subscribes to the\\nimage description topic and forwards the vision description,\\nalong with an LLM prompt, to ChatGPT 3.5. The prompt em-\\nployed is [”Consider the following ontology: You must write\\nyour reply with one short sentence. Behave as a CarMate that\\nsurveys the driver and gives him advice and instruction to drive\\nsafely. You will be given human language prompts describing\\nan image. Your task is to provide appropriate instructions to\\nthe driver based on the description.”]. Subsequently, ChatGPT\\n3.5 determines whether to caution the driver if they are at risk\\nor encourage them to maintain focus on the road. The lower\\nsection of Figure 5 presents two examples: the first involves a\\ndriver speaking on a cell phone while driving, and the second\\ndepicts a driver maintaining focus on driving.\\nB.Unified prompting through the YAML file\\nPresented below is the YAML configuration file used for\\nCarMate:\\n•Task_name : driver phone usage\\n•ROSGPT_Vision_Camera_Node :\\n–Image_Description_Method : llava # com-\\nment: you can choose between [llava, MiniGPT4,\\nSAM]\\n–Vision_prompt :”Describe the driver’s current\\nlevel of focus on driving based on the visual cues,\\nAnswer with one short sentence.”\\n–Choose_input :”video” # comment: you can\\nchoose between [webcam, video]\\n–Input_video :”Absolute path” # comment: if you\\nchose video\\n–Output_video :”Absolute path of output video\\ndemo”\\n•GPT_Consultation_Node :\\n–llm_prompt :”Consider the following ontology:\\nYou must write your Reply with one short sentence.\\nBehave as a carmate that surveys the driver and gives\\nhim advice and instruction to drive safely. You will\\nbe given human language prompts describing an im-\\nage. Your task is to provide appropriate instructions\\nto the driver based on the description.”', metadata={'source': 'docs/230811236.pdf', 'page': 7}), Document(page_content='9\\nThe driver is not \\nfocusing on the road \\nwhile driving \\nThe driver is focused \\non the road while \\ndriving Please, remind the \\ndriver to keep their \\neyes on the road\\nPlease, keep your \\neyes on the roadCase 1\\nCase 2LLM PromptPrompt Configuration\\nfor CarMate (YAML)\\nCarMate ApplicationVision Prompt\\nROSGPT_Vision Base\\nImage     \\nAcquisition\\nLLAVAImage Semantics\\nImage Description \\nTopicGPT Consultation \\nNode\\nGPT Consultation\\nTopicCamera Node\\nPublish Publish\\nFig. 5: CarMate Application diagram.\\n–GPT_temperature : 0.2\\n•MiniGPT4_parameters :\\n–configuration :”minigpt4 eval.yaml” # com-\\nment: absolute path for configuration of MiniGPT4\\nmodel\\n–temperature_miniGPT4 : 0.2\\n•llava_parameters :\\n–temperature_llavA : 0.2\\n–llama_version :”13B” # comment: you can\\nchoose between [7B, 13B]\\n•SAM_parameters :\\n–weights_SAM :”sam vith4b8939.pth” # com-\\nment: absolute path for configuration of MiniGPT4\\nmode\\nC. Prompting strategies\\nPrompting strategies for the CarMate application can be\\ndesigned to accurately describe the driver’s behavior and\\nprovide appropriate consultations. Here are some strategies for\\nboth visual and language model (LLM) prompts:\\n•Visual Prompts:\\n– Focused Description Prompts: These prompts can\\nbe designed to specifically describe the driver’s fo-\\ncus on driving. For example, ”Describe the driver’s\\ncurrent level of focus on driving based on the visual\\ncues.”– Behavioral Description Prompts: These prompts\\ncan be used to describe the driver’s overall behavior.\\nFor example, ”Describe the driver’s overall behavior,\\nincluding any distractions or signs of fatigue.”\\n– Ontological Prompts: These prompts can be de-\\nsigned to extract specific ontological entities from\\nthe visual data. For example, ”Identify and describe\\nthe ontological entities related to driving focus in the\\ncurrent scene.”\\n•LLM Prompts:\\n– Consultative Prompts: These prompts can be used\\nto generate appropriate consultations based on the\\nvisual descriptions. For example, ”Based on the vi-\\nsual description, provide a consultation to the driver\\nabout their current level of focus on driving.”\\n– Action-Oriented Prompts: These prompts can be\\nused to suggest actions that the driver should take\\nbased on the visual descriptions. For example, ”Sug-\\ngest actions the driver should take to improve their\\nfocus on driving based on the visual description.”\\n– Ontological Prompts: These prompts can be used to\\ngenerate consultations based on specific ontological\\nentities. For example, ”Provide a consultation to the\\ndriver based on the identified ontological entities\\nrelated to driving focus.”\\nIn this particular case, given the focus of the CarMate\\napplication on driver behavior monitoring and consultation,\\nan optimal approach would involve a combination of focused', metadata={'source': 'docs/230811236.pdf', 'page': 8}), Document(page_content='10\\nTABLE I: The Best Prompts in ten cases\\nVisual Prompts LLM Prompts\\nCases Focused Description Behavioral Description Ontological Consultative Action-Oriented Ontological\\nCase 1 ✓ ✓\\nCase 2 ✓ ✓\\nCase 3 ✓ ✓\\nCase 4 ✓ ✓\\nCase 5 ✓ ✓\\nCase 6 ✓ ✓\\nCase 7 ✓ ✓\\nCase 8 ✓ ✓\\nCase 9 ✓ ✓\\nCase 10 ✓ ✓\\ndescription prompts for visual cues and consultative prompts\\nfor the language model (LLM). However, determining the most\\neffective prompting strategy would ultimately depend on the\\nspecific requirements of the application and the capabilities of\\nthe models employed.\\nTo evaluate the best prompts for visual cues and LLM,\\nwe devised ten distinct scenarios that involved distracting the\\ndriver’s attention while driving. Table I presents the optimal\\nprompts for each scenario, and additional visual represen-\\ntations can be found in the appendix. Upon reviewing the\\nappendix and Table I, it becomes evident that Behavioral\\nDescription of Visual Prompts outperforms other prompt types\\ndue to its adaptability and ability to provide a clear description\\nfor each scenario. Conversely, Focused Description prompts,\\nwhile exhibiting high efficacy in some cases, generally fall\\nshort in delivering a precise description due to their limited\\nadaptability. Ontological prompts, on the other hand, offer\\nexcellent efficiency but provide a comprehensive image de-\\nscription without focusing on the driver’s behavior during\\ndriving, thereby impacting decision-making from the GPT\\nConsultation perspective.\\nRegarding LLM prompts, the Consultative prompt type ex-\\nhibits superiority over other prompt types due to its exclusive\\nfocus on describing the driver’s condition during driving, with-\\nout offering any directives. This aligns with the expected re-\\nsponsibilities of the CarMate application. Conversely, Action-\\nOriented Prompts enable direct decision-making without de-\\nscribing the driver’s behavior, while Ontological Prompts\\nprimarily offer assistance and advice to the driver in most\\ncases without providing any description of their behavior.\\nTherefore, based on our analysis, a combination of focused\\ndescription prompts for visual cues and consultative prompts\\nfor LLM is recommended for the CarMate application, taking\\ninto account the specific scenarios and requirements of the\\nsystem.\\nD.Description of the Datasets used in CarMate\\nIn this paper, we utilized two publicly available datasets,\\nnamely MDAD [42] and 3MDAD [43], to address the chal-\\nlenges of driver action recognition and driver distractionmonitoring. The MDAD dataset provides synchronized RGB\\nand depth data from side and frontal views, allowing re-\\nsearchers to develop algorithms across multiple modalities and\\nviews under different lighting and weather conditions. The\\n3MDAD dataset includes synchronized data modalities from\\ndaytime and nighttime recordings, offering a comprehensive\\ndataset for evaluating driver distraction monitoring methods.\\nBoth datasets enable an independent analysis of recognition\\nresults based on individual modalities as well as combinations\\nof modalities. These datasets provide valuable resources for\\nadvancing research in the field of driver behavior analysis and\\nimproving road safety.\\nV. C ONCLUSION\\nIn this paper, we have introduced a new Robotic design\\npattern: the Prompting Robotic Modalities (PRM). It opts for\\ndesigning robotic application as Robotic Modalities connected\\nto Task Modality. Every Modality is commanded via a spe-\\ncific prompt. Every Robotic Modality incorporates a specific\\nModality Language Model (MLM) while the Task Modality\\nis connected to a Large Language Model (LLM). The PRM\\ndesign pattern is used to conceive a new robotic framework:\\nthe ROSGPT Vision, a novel robotics framework that connects\\nlanguage models to computer vision capabilities, enabling\\nenhanced human-robot interaction and understanding. By in-\\ntegrating LLMs with ROS and VLMs, ROSGPT Vision tries\\nto make the robots interact with visual data through natural\\nlanguage. The Visual Robotic Modality of ROSGPT Vision\\nlies in the Image Semantics module, which effectively extracts\\nmeaning from images and generates descriptive narrations in\\nhuman-like language. It integrates advanced VLM models such\\nas LLA V A, MiniGPT-4, and SAM.\\nThis paper tested the ROSGPT Vision framework in the\\ndevelopment of the CarMate application, a driver monitoring\\nand assistance system designed to ensure safe and efficient\\ndriving experiences. By continuously monitoring driver ac-\\ntions, expressions, and attention levels, CarMate can identify\\nunsafe behaviors and issue timely alerts to prompt corrective\\nactions. Additionally, CarMate generates personalized driving\\nreports, providing valuable insights to drivers for developing\\nsafer driving habits. The whole development of the CarMate\\nis reduced remarkably due to the PRM and ROSGPT Vision,\\nit needs mainly the setting of only two prompts in the YAML\\nfile.\\nTo optimize the performance of CarMate, we have explored\\nvarious prompting strategies for both visual and language\\nmodel prompts. Through thorough evaluation and analysis,\\nwe have identified the most effective prompt types, enabling\\nCarMate to deliver precise and contextually appropriate con-\\nsultations to drivers.\\nIn conclusion, PRM and ROSGPT Vision represents a new\\ncontribution to the realm of robotics. By releasing the open-\\nsource implementation of ROSGPT Vision, we invite the\\nrobotics community to collaborate and enhance this inter-\\ndisciplinary research field further. The successful integration\\nof LLMs with Image data analysis within the ROS system\\nincreases the potential for innovative applications, such as', metadata={'source': 'docs/230811236.pdf', 'page': 9}), Document(page_content='11\\nvirtual assistants and intelligent robots that can seamlessly\\ninteract with humans and their environment.\\nREFERENCES\\n[1] A. Koubaa, “Rosgpt: Next-generation human-robot interaction with\\nchatgpt and ros,” Preprints.org , vol. 2023, p. 2023040827, 2023.\\n[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems , vol. 33, pp. 1877–1901, 2020.\\n[3] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\\nlanguage understanding by generative pre-training,” URL https://s3-\\nus-west-2. amazonaws. com/openai-assets/research-covers/language-\\nunsupervised/language understanding paper. pdf , 2018.\\n[4] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\\nJ. Zhang, Z. Dong et al. , “A survey of large language models,” arXiv\\npreprint arXiv:2303.18223 , 2023.\\n[5] A. Koubaa, W. Boulila, L. Ghouti, A. Alzahem, and S. Latif, “Exploring\\nChatGPT Capabilities and Limitations: A Critical Review of the NLP\\nGame Changer,” Preprints.org , vol. 2023, no. 2023030438, 2023.\\n[Online]. Available: https://doi.org/10.20944/preprints202303.0438.v1\\n[6] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, “Vision-and-\\ndialog navigation,” in Conference on Robot Learning . PMLR, 2020,\\npp. 394–406.\\n[7] H. Rashkin, E. M. Smith, M. Li, and Y .-L. Boureau, “Towards em-\\npathetic open-domain conversation models: A new benchmark and\\ndataset,” arXiv preprint arXiv:1811.00207 , 2018.\\n[8] S. Wachter, B. D. Mittelstadt, and L. Floridi, “Transparent, explainable,\\nand accountable ai for robotics,” Science Robotics , vol. 2, 2017.\\n[9] N. Gao, Z. Zhao, Z. Zeng, S. Zhang, and D. Weng, “Gesgpt:\\nSpeech gesture synthesis with text parsing from gpt,” arXiv preprint\\narXiv:2303.13013 , 2023.\\n[10] OpenAI, “Gpt-4 technical report,” ArXiv , vol. abs/2303.08774, 2023.\\n[11] A. Koubaa, “GPT-4 vs. GPT-3.5: A Concise Showdown,” Preprints.org ,\\nvol. 2023030422, 2023. [Online]. Available: https://www.preprints.org/\\nmanuscript/202303.0422/v1\\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805 , 2018.\\n[13] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\\nlearning with a unified text-to-text transformer,” The Journal of Machine\\nLearning Research , vol. 21, no. 1, pp. 5485–5551, 2020.\\n[14] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning\\nhand-eye coordination for robotic grasping with deep learning and large-\\nscale data collection,” The International Journal of Robotics Research ,\\nvol. 37, no. 4-5, pp. 421–436, 2018.\\n[15] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma,\\nO. Taylor, M. Liu, E. Romo et al. , “Robotic pick-and-place of novel\\nobjects in clutter with multi-affordance grasping and cross-domain image\\nmatching,” The International Journal of Robotics Research , vol. 41,\\nno. 7, pp. 690–705, 2022.\\n[16] A. Saxena, J. Driemeyer, and A. Y . Ng, “Robotic grasping of novel\\nobjects using vision,” The International Journal of Robotics Research ,\\nvol. 27, no. 2, pp. 157–173, 2008.\\n[17] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea,\\nand K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps\\nwith synthetic point clouds and analytic grasp metrics,” arXiv preprint\\narXiv:1703.09312 , 2017.\\n[18] N. S ¨underhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner,\\nB. Upcroft, P. Abbeel, W. Burgard, M. Milford et al. , “The limits and\\npotentials of deep learning for robotics,” The International journal of\\nrobotics research , vol. 37, no. 4-5, pp. 405–420, 2018.\\n[19] A. I. K ´aroly, P. Galambos, J. Kuti, and I. J. Rudas, “Deep learning\\nin robotics: Survey on model structures and training strategies,” IEEE\\nTransactions on Systems, Man, and Cybernetics: Systems , vol. 51, no. 1,\\npp. 266–279, 2020.\\n[20] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic\\ngrasps,” The International Journal of Robotics Research , vol. 34, no.\\n4-5, pp. 705–724, 2015.\\n[21] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\\nonce: Unified, real-time object detection,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2016, pp. 779–\\n788.[22] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement\\nlearning for robotic manipulation with asynchronous off-policy updates,”\\nin2017 IEEE international conference on robotics and automation\\n(ICRA) . IEEE, 2017, pp. 3389–3396.\\n[23] J. F. Gorostiza, R. Barber, A. M. Khamis, M. Malfaz, R. Pacheco,\\nR. Rivas, A. C. Paredes, E. Delgado, and M. A. Salichs, “Multimodal\\nhuman-robot interaction framework for a personal robot,” ROMAN\\n2006 - The 15th IEEE International Symposium on Robot and Human\\nInteractive Communication , pp. 39–44, 2006.\\n[24] J. Bohg, K. Hausman, B. Sankaran, O. Brock, D. Kragic, S. Schaal, and\\nG. S. Sukhatme, “Interactive perception: Leveraging action in perception\\nand perception in action,” IEEE Transactions on Robotics , vol. 33, no. 6,\\npp. 1273–1291, 2017.\\n[25] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to grasp\\nfrom 50k tries and 700 robot hours,” in 2016 IEEE international\\nconference on robotics and automation (ICRA) . IEEE, 2016, pp. 3406–\\n3413.\\n[26] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-\\nagnostic visiolinguistic representations for vision-and-language tasks,”\\narXiv preprint arXiv:1908.02265 , 2019.\\n[27] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\\nL. Dong, F. Wei et al. , “Oscar: Object-semantics aligned pre-training for\\nvision-language tasks,” in Computer Vision–ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX\\n16. Springer, 2020, pp. 121–137.\\n[28] F. Li, H. Zhang, Y .-F. Zhang, S. Liu, J. Guo, L. M. Ni, P. Zhang, and\\nL. Zhang, “Vision-language intelligence: Tasks, representation learning,\\nand large models,” arXiv preprint arXiv:2203.01922 , 2022.\\n[29] J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for\\nvision tasks: A survey,” arXiv preprint arXiv:2304.00685 , 2023.\\n[30] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\\nL. Zhang, “Bottom-up and top-down attention for image captioning and\\nvisual question answering,” in Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , 2018, pp. 6077–6086.\\n[31] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder\\nrepresentations from transformers,” arXiv preprint arXiv:1908.07490 ,\\n2019.\\n[32] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert:\\nPre-training of generic visual-linguistic representations,” arXiv preprint\\narXiv:1908.08530 , 2019.\\n[33] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4:\\nEnhancing vision-language understanding with advanced large language\\nmodels,” arXiv preprint arXiv:2304.10592 , 2023.\\n[34] Q. Zhang, J. Zhang, Y . Xu, and D. Tao, “Vision transformer with\\nquadrangle attention,” arXiv preprint arXiv:2303.15105 , 2023.\\n[35] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,” arXiv\\npreprint arXiv:2304.08485 , 2023.\\n[36] S. Long, F. Cao, S. C. Han, and H. Yang, “Vision-and-language\\npretrained models: A survey,” arXiv preprint arXiv:2204.07356 , 2022.\\n[37] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\\n“A survey on bias and fairness in machine learning,” ACM Computing\\nSurveys (CSUR) , vol. 54, no. 6, pp. 1–35, 2021.\\n[38] M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust you?”\\nexplaining the predictions of any classifier,” in Proceedings of the 22nd\\nACM SIGKDD international conference on knowledge discovery and\\ndata mining , 2016, pp. 1135–1144.\\n[39] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for\\nrobotics: Design principles and model abilities,” Microsoft Auton. Syst.\\nRobot. Res , vol. 2, p. 20, 2023.\\n[40] H. M. He, “Robotgpt: From chatgpt to robot intelligence,”\\nopenreview.net , 2023. [Online]. Available: openreview.net\\n[41] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y . Lo et al. , “Segment anything,”\\narXiv preprint arXiv:2304.02643 , 2023.\\n[42] I. Jegham, A. Ben Khalifa, I. Alouani, and M. A. Mahjoub, “Mdad: A\\nmultimodal and multiview in-vehicle driver action dataset,” in Computer\\nAnalysis of Images and Patterns: 18th International Conference, CAIP\\n2019, Salerno, Italy, September 3–5, 2019, Proceedings, Part I 18 .\\nSpringer, 2019, pp. 518–529.\\n[43] I. Jegham, A. B. Khalifa, I. Alouani, and M. A. Mahjoub, “A novel pub-\\nlic dataset for multimodal multiview and multispectral driver distraction\\nanalysis: 3mdad,” Signal Processing: Image Communication , vol. 88, p.\\n115960, 2020.', metadata={'source': 'docs/230811236.pdf', 'page': 10}), Document(page_content='12\\nAPPENDIX\\nA. Prompting strategies\\n1) Case 1: Drinking Coffee during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 11}), Document(page_content='13\\n2) Case 2: Focus on the road during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 12}), Document(page_content='14\\n3) Case 3: holding a cup during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 13}), Document(page_content='15\\n4) Case 4: looking down during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 14}), Document(page_content='16\\n5) Case 5: looking to passenger during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 15}), Document(page_content='17\\n6) Case 6: using radio during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 16}), Document(page_content='18\\n7) Case 7: using mobile during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 17}), Document(page_content='19\\n8) Case 8: sleeping during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 18}), Document(page_content='20\\n9) Case 9: smoking during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 19}), Document(page_content='21\\n10) Case 10: speaking in mobile during driving\\na) Visual Prompts\\nFocused Description\\nBehavioral Description\\nOntological\\nb) LLM Prompts\\nConsultative\\nAction-Oriented\\nOntological\\n', metadata={'source': 'docs/230811236.pdf', 'page': 20})]\n"
     ]
    }
   ],
   "source": [
    "print({len(data)})\n",
    "# print({len(data[0].page_content)})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a splitter and split the text into multiple chunks.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) \n",
    "texts = loader.load_and_split(splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='and makes real-time vocal notifications to the driver. We showed\\nhow ROSGPT Vision significantly reduced the development cost\\ncompared to traditional methods. We demonstrated how to im-\\nprove the quality of the application by optimizing the prompting\\nstrategies, without delving into technical details. ROSGPT Vision\\nis shared with the community1to advance robotic research in this\\ndirection and to build more robotic frameworks that implement\\nthe PRM design pattern and enables controlling robots using\\nonly prompts.\\nIndex Terms —Robotic Design Patterns, Robotic Modalities,\\nPrompting Robotic Modalities, Large Language Models, LLMs,\\nVision Language Models, VLMs, Robotic Operating System,\\nROS, ROS2, Robotic Prompt Engineering, Visual prompt, LLM\\nprompt\\nI. I NTRODUCTION\\nRecent advances in Large Language Models (LLMs),\\nRobotics, and Computer Vision have developed new research\\ntrends to bridge the gap between visual and sensor data\\ninterpretation, language modeling, and robotic actions. This', metadata={'source': 'docs/230811236.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(texts))\n",
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/anaconda3/envs/morris_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7f7b2ee43910>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7f7b2efe0070>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use OpenAIEmbeddings \n",
    "import getpass\n",
    "import os\n",
    "embeddings =OpenAIEmbeddings(api_key=OpenAIKey)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0309263641403164,\n",
       " -0.020379746726595614,\n",
       " -0.019380867855759716,\n",
       " -0.0417454027256024,\n",
       " -0.02492011014100518,\n",
       " 0.02438823925894467,\n",
       " -0.018005784948928193,\n",
       " -0.017668500884077863,\n",
       " -0.006463531995300285,\n",
       " -0.015891272881756072,\n",
       " 0.02589304608970125,\n",
       " -0.006959729031112052,\n",
       " -0.01796686870307327,\n",
       " -0.011817918387444486,\n",
       " 0.011389826644524023,\n",
       " 0.016513951441886714,\n",
       " 0.03886551485065948,\n",
       " 0.0005270065259516295,\n",
       " 0.03209388690622265,\n",
       " -0.008691553194398776,\n",
       " -0.019770041490180072,\n",
       " -0.004841973633011155,\n",
       " -0.00937260798595699,\n",
       " -0.014308631696644458,\n",
       " -0.022896405751903187,\n",
       " 0.002472876618906248,\n",
       " 0.010040690385122696,\n",
       " -0.011798459333194431,\n",
       " 0.002571791692975724,\n",
       " -0.02595790898298637,\n",
       " 0.014503219445177228,\n",
       " 0.0007438899627285669,\n",
       " -0.035752122050006274,\n",
       " -0.014892393079597582,\n",
       " -0.009476388056419626,\n",
       " -0.02471255186272509,\n",
       " 0.006959729031112052,\n",
       " -0.021145122534366408,\n",
       " 0.01913438960633433,\n",
       " -0.005623564698441934,\n",
       " 0.006184624161877525,\n",
       " -0.0007094318417091566,\n",
       " 0.0013621091174551288,\n",
       " -0.01452916422996224,\n",
       " -0.02297424010625822,\n",
       " -0.006019224994719838,\n",
       " 0.0010272573179707298,\n",
       " 0.004086327352365387,\n",
       " -0.012408165500932567,\n",
       " 0.01989976541410513,\n",
       " 0.015904246205471168,\n",
       " 0.007044050047324634,\n",
       " -0.029732896589625142,\n",
       " -0.011415771429309034,\n",
       " -0.020185159909385436,\n",
       " -0.007725104838882847,\n",
       " -0.015164815182485384,\n",
       " 0.011175780773063792,\n",
       " -0.010533644090005687,\n",
       " -0.022675874149908,\n",
       " -0.010663368945253337,\n",
       " 0.005552216074621857,\n",
       " -0.004880891275849968,\n",
       " 0.006028954521844865,\n",
       " 0.01042986401954305,\n",
       " -0.008477507322938544,\n",
       " 0.023635836774888973,\n",
       " -0.0010410405430954291,\n",
       " 0.00455982246865962,\n",
       " 0.000731322870286834,\n",
       " 0.02149537992293184,\n",
       " 0.03375436058304411,\n",
       " -0.005023588057828826,\n",
       " -0.014295659304251952,\n",
       " 0.012148715790437269,\n",
       " -0.007329444542604944,\n",
       " -0.0035512131393761032,\n",
       " -0.0011780621944408765,\n",
       " -0.0061327341266462065,\n",
       " -0.006920811853934535,\n",
       " 0.02517955985150048,\n",
       " -0.031367429206951965,\n",
       " -0.015969107236111106,\n",
       " 0.017460942605797773,\n",
       " 0.008295892898120874,\n",
       " 0.016851235506737044,\n",
       " -0.005137097189755195,\n",
       " 0.012920578260065613,\n",
       " -0.015605879317798355,\n",
       " -0.03188632490265219,\n",
       " 0.006447316737640301,\n",
       " 0.02294829532147321,\n",
       " 0.013517311104088651,\n",
       " 0.01384162370786907,\n",
       " -0.011039570187281186,\n",
       " 0.01833009755270861,\n",
       " -0.011597385854126705,\n",
       " 0.01851171104620369,\n",
       " -0.00742025175501378,\n",
       " 0.00632732140951768,\n",
       " 0.0008423996204319459,\n",
       " 0.008989920082071591,\n",
       " 0.0013304887363754102,\n",
       " -0.005500326505051834,\n",
       " -0.02791675047880324,\n",
       " -0.020561362082735877,\n",
       " -0.017422024497297663,\n",
       " -0.013893513277439092,\n",
       " 0.01677340115238201,\n",
       " -0.0014804828965286872,\n",
       " -0.0027907020951678215,\n",
       " 0.03655641596627718,\n",
       " 0.005487354112659329,\n",
       " -0.0342732600040347,\n",
       " -0.010488240250970622,\n",
       " -0.00790671926370052,\n",
       " 0.003187984056910112,\n",
       " -0.0033339241698177448,\n",
       " -0.012233036806649852,\n",
       " -0.02322071835568361,\n",
       " 0.004569551995784647,\n",
       " 0.00018911426379257493,\n",
       " 0.015631823171260772,\n",
       " 0.0034085158917359485,\n",
       " 0.015696686064545894,\n",
       " 0.008016985064698112,\n",
       " -0.02411581808737946,\n",
       " -0.0052960101607166295,\n",
       " 0.0075694351988501865,\n",
       " 0.0049943999421150406,\n",
       " 0.04810191038818868,\n",
       " 0.011843863172229498,\n",
       " 0.005201959617379019,\n",
       " -0.007154315848322229,\n",
       " -0.03359869187433404,\n",
       " 0.03536294841558592,\n",
       " -0.024868220571435156,\n",
       " 0.023052076323258442,\n",
       " -0.02394717605495429,\n",
       " -0.032457113893212805,\n",
       " -0.005643023752691989,\n",
       " 0.03829472586009886,\n",
       " -7.261541899689028e-05,\n",
       " -0.017383106388797552,\n",
       " -0.013789733206976454,\n",
       " -0.013426504357341112,\n",
       " 0.01511292468159277,\n",
       " -0.002947993400664869,\n",
       " -0.001598045842870052,\n",
       " -0.0021145123000027707,\n",
       " 0.001618315380606328,\n",
       " 0.001832361135651236,\n",
       " 0.020107325555030402,\n",
       " -0.00047552202350173146,\n",
       " 0.02942155730955982,\n",
       " 0.02067811454559102,\n",
       " -0.012985440222028142,\n",
       " -0.010793092869178393,\n",
       " -0.010877413885390977,\n",
       " -0.008834250442038932,\n",
       " -0.0007430791882040353,\n",
       " -0.0030306927514130643,\n",
       " 0.02383042359209915,\n",
       " -0.010689313730038348,\n",
       " -0.003791203795621346,\n",
       " 0.017629584638222937,\n",
       " 0.012233036806649852,\n",
       " 0.027345961488242622,\n",
       " 0.012888146813423053,\n",
       " -0.0018372257827984255,\n",
       " 0.013387587180163594,\n",
       " 0.019601399457754905,\n",
       " -0.030277740795400746,\n",
       " 0.0053608725883404545,\n",
       " 0.0004516040295088941,\n",
       " 0.010981193955853613,\n",
       " 0.015787493742616027,\n",
       " 0.013634063566943794,\n",
       " -0.03035557514975578,\n",
       " -0.04408044453212452,\n",
       " -0.01526859432162543,\n",
       " 0.017214464356372385,\n",
       " 0.032742508388493116,\n",
       " 0.029551281233484876,\n",
       " -0.009236397400174384,\n",
       " 0.014269714519466941,\n",
       " 0.029836675728765187,\n",
       " 0.0048290012406186494,\n",
       " 0.016591785796241747,\n",
       " -0.020198133233100535,\n",
       " 0.015592905994083256,\n",
       " 0.029032383675139467,\n",
       " 0.0169939327543772,\n",
       " -0.006586770654351681,\n",
       " -0.7011359469483905,\n",
       " -0.03118581198816651,\n",
       " -0.0045760381919809,\n",
       " -0.0019393839549660277,\n",
       " 0.020820811793231177,\n",
       " 0.04299075984586368,\n",
       " 0.02669733814332697,\n",
       " 0.02146943513814683,\n",
       " -0.018213345089853467,\n",
       " 0.015969107236111106,\n",
       " -0.007815911585630386,\n",
       " -0.004601982976765911,\n",
       " 0.008244003328550851,\n",
       " -0.014879420687205077,\n",
       " 0.0055651889326756585,\n",
       " 0.007407279362621274,\n",
       " -0.00034579734287706865,\n",
       " -0.007407279362621274,\n",
       " -0.011247129396883868,\n",
       " 0.0019588426599701104,\n",
       " -0.004070111629044106,\n",
       " -0.003979304416635271,\n",
       " -0.02345422141874871,\n",
       " 0.020781893684731066,\n",
       " 0.009340176539314429,\n",
       " -0.005435464077428009,\n",
       " -0.0022312645300272657,\n",
       " -0.009327204146921923,\n",
       " -0.019510591779684772,\n",
       " 0.010261221987117885,\n",
       " -0.013763788422191442,\n",
       " 0.011947643242692135,\n",
       " 0.012044936651297224,\n",
       " -0.010352029665188016,\n",
       " 0.05988091346110083,\n",
       " 0.005078720958327623,\n",
       " -0.004339290401003135,\n",
       " 0.02026299426374047,\n",
       " 0.006414885290997741,\n",
       " 0.0491656521523097,\n",
       " -0.0006356509525231466,\n",
       " -0.01934194974725961,\n",
       " 0.005276551106466575,\n",
       " 0.00632732140951768,\n",
       " 0.005707885714654517,\n",
       " 0.016903125076307066,\n",
       " -0.009048296313499163,\n",
       " -0.0015129141103405995,\n",
       " 0.009547736680239704,\n",
       " -0.0009932045551097545,\n",
       " 0.010851469100605965,\n",
       " -0.00670027978627805,\n",
       " -0.012297899699934972,\n",
       " 0.018913858004339142,\n",
       " 0.009411525163134506,\n",
       " -0.002793945193265948,\n",
       " 0.021923471665852304,\n",
       " -0.024466073613299704,\n",
       " -0.005954362567096013,\n",
       " 0.023999065624524314,\n",
       " 0.0004309291627180142,\n",
       " 0.0030533946709305976,\n",
       " -0.014127017271826785,\n",
       " -0.019290060177689587,\n",
       " -0.013880540885046587,\n",
       " 0.019783012951249984,\n",
       " -0.022001306020207338,\n",
       " -0.009586653857417221,\n",
       " -0.0007232150668338559,\n",
       " -0.005977064486613546,\n",
       " -0.013134624131525845,\n",
       " 0.019290060177689587,\n",
       " -0.02742379770524284,\n",
       " -0.016384227517961654,\n",
       " 0.001965328856166363,\n",
       " 0.02389528648538427,\n",
       " 0.012427624555182622,\n",
       " -0.006100303145664942,\n",
       " -0.0091520763839618,\n",
       " 0.010125011401335279,\n",
       " 0.0013191377766166438,\n",
       " -0.002440445637924984,\n",
       " -0.005406275961714224,\n",
       " -0.011402799036916529,\n",
       " 0.02414176287216447,\n",
       " -0.026593559004186924,\n",
       " -0.033650581443904065,\n",
       " 0.014412411767107095,\n",
       " -0.00674568315965182,\n",
       " -0.002550711438922578,\n",
       " 0.0055619456017468845,\n",
       " 0.0358040116195763,\n",
       " 0.013004899276278197,\n",
       " -0.020224078017885547,\n",
       " 0.006126247930449954,\n",
       " 0.009450443271634615,\n",
       " -0.02049649918945076,\n",
       " 0.002980424614476781,\n",
       " -0.0031831192933475983,\n",
       " -0.012693559996212876,\n",
       " -0.007945636440878036,\n",
       " 0.0028166471127834807,\n",
       " -0.0016466926635879202,\n",
       " -0.005659239010351973,\n",
       " 0.016721511582811988,\n",
       " -0.0038463366961201434,\n",
       " -0.014477273729069623,\n",
       " 0.03985142039778028,\n",
       " 0.027501632059597873,\n",
       " -0.020483527728380843,\n",
       " -0.0024988216365219073,\n",
       " -0.0011804945762221332,\n",
       " -0.007822398247487936,\n",
       " 0.005493840308855582,\n",
       " 0.00830886529051338,\n",
       " -0.033001959961633597,\n",
       " -0.012051422381832181,\n",
       " 0.03147120834609201,\n",
       " 0.02167699341642692,\n",
       " -0.015216704752055407,\n",
       " 0.005201959617379019,\n",
       " -0.0037003965832125103,\n",
       " 0.021067288180011374,\n",
       " -0.016565841011456736,\n",
       " 0.0014310253594939492,\n",
       " 0.018615492047988923,\n",
       " -0.010488240250970622,\n",
       " -0.003538240514152949,\n",
       " -0.0316787666243721,\n",
       " 0.007874287817057958,\n",
       " 0.013919458062224103,\n",
       " -0.011357395197881462,\n",
       " 0.017759308562147993,\n",
       " -0.020587306867520888,\n",
       " 0.01424376973468193,\n",
       " -0.005996523075202305,\n",
       " 0.0011958993503958957,\n",
       " -0.01080606619289349,\n",
       " -0.008159682312338268,\n",
       " -0.005172771501665233,\n",
       " -0.0021388356521540425,\n",
       " 0.004653873011997231,\n",
       " 6.258202604814966e-05,\n",
       " -0.004631171092479697,\n",
       " 0.0019961384045138883,\n",
       " -0.03227549853707254,\n",
       " -0.01974409670539506,\n",
       " -0.005056019504471387,\n",
       " -0.012687073334355327,\n",
       " 0.004974941353526282,\n",
       " 0.0005943011607337246,\n",
       " -0.001832361135651236,\n",
       " -0.01083849670821346,\n",
       " 0.018848995111054024,\n",
       " 0.03546672755472596,\n",
       " -0.02205319558977736,\n",
       " -0.0035025664350735587,\n",
       " -0.021365654136361597,\n",
       " -0.011273074181668879,\n",
       " -5.6349156873045314e-05,\n",
       " 0.0034571625960384926,\n",
       " 0.022377506330912594,\n",
       " -0.014944282649167605,\n",
       " 0.002070730126432092,\n",
       " 0.0017739850206389884,\n",
       " -0.02760541119873792,\n",
       " -0.001626423125851644,\n",
       " 0.023402331849178688,\n",
       " 0.0030079910647261795,\n",
       " -0.03837255835180871,\n",
       " 0.017175548110517462,\n",
       " -0.01581343852740104,\n",
       " -0.006158678911431218,\n",
       " 0.0026123307684482763,\n",
       " 0.017772281885863092,\n",
       " 0.006129491261378728,\n",
       " -0.007491600378833857,\n",
       " 0.00749808657503011,\n",
       " -0.0014796720637964935,\n",
       " -0.032898177097203184,\n",
       " 0.01070877185296581,\n",
       " 0.0021226201616634105,\n",
       " -0.010676341337645842,\n",
       " 0.010170415240370346,\n",
       " 0.021041343395226363,\n",
       " 0.008004012672305607,\n",
       " 0.008873167619216448,\n",
       " 0.02569845927249107,\n",
       " -0.012745449565782899,\n",
       " 0.01090335960149858,\n",
       " 0.007530517556011374,\n",
       " 0.02054838875902078,\n",
       " -0.017188519571587374,\n",
       " -0.0012615724943365898,\n",
       " 0.008198599489515785,\n",
       " 0.011299019897776484,\n",
       " 0.0025215235560394405,\n",
       " 0.002695030119196472,\n",
       " 0.009132617329711745,\n",
       " 0.023869341700599258,\n",
       " 0.04218646592959277,\n",
       " 0.009482873786954582,\n",
       " 0.0342992066514649,\n",
       " -0.011156322650136328,\n",
       " -0.009670974873629804,\n",
       " -0.026230330154551583,\n",
       " 0.001070228658809215,\n",
       " -0.005701399518458264,\n",
       " 0.020691086006660933,\n",
       " 0.016267475055106513,\n",
       " -0.005208445813575272,\n",
       " -0.01517778757487789,\n",
       " 0.00868506746386382,\n",
       " 0.003907955792815193,\n",
       " 0.019251142069189476,\n",
       " 0.017798226670648103,\n",
       " -0.008023471726555662,\n",
       " 0.012991926883885691,\n",
       " -0.02098945382565634,\n",
       " 0.010313111556687907,\n",
       " 0.011882780349407015,\n",
       " -0.011046056849138735,\n",
       " 0.014049182917471752,\n",
       " -0.00016458818890633886,\n",
       " 0.00493278084541999,\n",
       " 0.01615072259225137,\n",
       " 0.024025010409309325,\n",
       " 0.033858139722184155,\n",
       " 0.013387587180163594,\n",
       " -0.00830886529051338,\n",
       " -0.001671826848471386,\n",
       " -0.0036387772536868126,\n",
       " 0.008639663624828754,\n",
       " 0.004747923089673544,\n",
       " -0.008451562538153533,\n",
       " 0.0025312528503338197,\n",
       " 0.0013491366086472991,\n",
       " -0.025283340853285707,\n",
       " 0.026217356830836484,\n",
       " -0.010883900547248524,\n",
       " 0.0026544912765545676,\n",
       " 0.02760541119873792,\n",
       " 0.029162107599064523,\n",
       " -0.02695678785382227,\n",
       " 0.0069662152273083046,\n",
       " 0.01969220713582504,\n",
       " 0.009748809227984838,\n",
       " 0.004216052207613036,\n",
       " 0.019186279175904354,\n",
       " 0.025244422744785597,\n",
       " 0.006392183371480208,\n",
       " 0.011227671273956406,\n",
       " -0.0017010148477698479,\n",
       " 0.005915445389918496,\n",
       " 0.01849873958513378,\n",
       " -0.004841973633011155,\n",
       " -0.016825290721952033,\n",
       " 0.004728464501084786,\n",
       " 0.0091520763839618,\n",
       " 0.018252263198353578,\n",
       " 0.004313345616218124,\n",
       " 0.018420905230778745,\n",
       " 0.015216704752055407,\n",
       " -0.023856368376884162,\n",
       " 0.01858954726320391,\n",
       " 0.0008691553543644749,\n",
       " -0.004978184684455056,\n",
       " 0.002185860923822848,\n",
       " 0.008250489990408399,\n",
       " 0.0009834752608153753,\n",
       " -0.005244120125485311,\n",
       " -0.0026804362941702273,\n",
       " 0.026853008714682224,\n",
       " -0.016864208830452143,\n",
       " 0.03160093040737188,\n",
       " 0.013017871668670702,\n",
       " -0.0008918571574666838,\n",
       " 0.0016410171837085371,\n",
       " 0.0006311916635343917,\n",
       " -0.010189873363297807,\n",
       " -0.020846756578016188,\n",
       " -0.03481810234716513,\n",
       " 0.01561885171019086,\n",
       " 0.010514185035755633,\n",
       " -0.004618198700087192,\n",
       " -0.023635836774888973,\n",
       " -0.03912496269850959,\n",
       " 0.009009379136321646,\n",
       " 0.009080727760141722,\n",
       " 0.006071115029951156,\n",
       " -0.004790083597779836,\n",
       " 0.02276668182797813,\n",
       " 0.030303685580185757,\n",
       " -0.022468314008982724,\n",
       " 0.002061000832137713,\n",
       " 0.0020755948899946054,\n",
       " 0.039306578054649856,\n",
       " -0.007686187196044034,\n",
       " 0.0010613100808317052,\n",
       " 0.007725104838882847,\n",
       " 0.0009526655960525261,\n",
       " 0.004170648368577969,\n",
       " 0.005493840308855582,\n",
       " -0.005253849652610338,\n",
       " -0.0059316611132397765,\n",
       " 0.017538776960152807,\n",
       " -0.00836724152194095,\n",
       " -0.02117106731915142,\n",
       " -0.007322958346408691,\n",
       " 0.016384227517961654,\n",
       " -0.004167405503310491,\n",
       " 0.032716565466353285,\n",
       " -0.009534764287847199,\n",
       " 0.01517778757487789,\n",
       " 0.0004795759252282205,\n",
       " 0.007666728607455276,\n",
       " -0.009301259362136912,\n",
       " -0.002203698079777867,\n",
       " 0.02008138077024539,\n",
       " -0.010598506051968215,\n",
       " -0.025490899131565797,\n",
       " -0.008542370216223664,\n",
       " -0.018343069013778524,\n",
       " -0.0015542637857146973,\n",
       " 0.054017354846784583,\n",
       " 0.021975361235422326,\n",
       " -0.006145706519038712,\n",
       " 0.0017739850206389884,\n",
       " 0.010942276778676097,\n",
       " 0.019316004962474598,\n",
       " -0.0001681353492790379,\n",
       " -0.014204852557504413,\n",
       " -0.013173541308703362,\n",
       " -0.01439943937471459,\n",
       " 0.010228790540475324,\n",
       " -0.017499858851652696,\n",
       " -0.0136470359593363,\n",
       " 0.0036679653694005982,\n",
       " 0.004430097613411971,\n",
       " 0.00165561124156543,\n",
       " 0.020211104694170447,\n",
       " -0.006804059391079391,\n",
       " 0.012550862748572722,\n",
       " -0.00290421122709419,\n",
       " -0.0063370504709814104,\n",
       " 0.0029285345792454624,\n",
       " 0.011577927731199244,\n",
       " 0.025815211735346216,\n",
       " 0.015333457214910551,\n",
       " 0.012077367166617193,\n",
       " 0.022792626612763142,\n",
       " 0.018939802789124154,\n",
       " 0.010254736256582927,\n",
       " -0.026074659583196328,\n",
       " -0.008490480646653642,\n",
       " -0.0022912621940885764,\n",
       " 0.013621091174551288,\n",
       " 0.006408399094801488,\n",
       " -0.0011123891669155065,\n",
       " 0.027761081770093173,\n",
       " -0.010994166348246119,\n",
       " 0.010520671697613181,\n",
       " 0.022507232117482835,\n",
       " -0.012142230059902313,\n",
       " 0.006810545587275644,\n",
       " 0.016967987969592188,\n",
       " 0.005347899730286652,\n",
       " -0.017032850862877306,\n",
       " 0.018031729733713205,\n",
       " -0.01935492120832952,\n",
       " -0.019783012951249984,\n",
       " 0.03227549853707254,\n",
       " -0.009755295889842386,\n",
       " -0.009223425007781878,\n",
       " 0.028409705115008826,\n",
       " 0.0009713135265320771,\n",
       " -0.016163695915966468,\n",
       " -0.01049472691282817,\n",
       " -0.0016799345937167022,\n",
       " 0.006113275538057448,\n",
       " -0.020094352231315303,\n",
       " -0.011337937074954,\n",
       " -0.009054782975356711,\n",
       " 0.007647270018866517,\n",
       " -0.010864441492998471,\n",
       " -0.020120297016100314,\n",
       " 0.007751049623667858,\n",
       " -0.006499206307210324,\n",
       " -0.028305924113223593,\n",
       " -0.03191227155008238,\n",
       " -0.02763135598352293,\n",
       " -0.011830890779836992,\n",
       " -0.015605879317798355,\n",
       " 0.003719855171801269,\n",
       " -0.016111804483751258,\n",
       " -0.001267247974215973,\n",
       " -0.012018991866512212,\n",
       " -0.005600863244585697,\n",
       " 0.0018145239796962167,\n",
       " 0.017409051173582563,\n",
       " 0.00493278084541999,\n",
       " -0.013660008351728805,\n",
       " 0.024504991721799815,\n",
       " 0.007050536243520887,\n",
       " -0.012252495860899907,\n",
       " -0.030459354288895825,\n",
       " 0.002109647536440257,\n",
       " -0.0183041527679236,\n",
       " 0.013076247900098275,\n",
       " 0.009904478868017498,\n",
       " -0.009547736680239704,\n",
       " -0.00491656512209871,\n",
       " -0.008749929425826348,\n",
       " 0.000897532637346067,\n",
       " 0.02555576202485092,\n",
       " 0.011052542579673691,\n",
       " 0.010942276778676097,\n",
       " -0.008626691232436248,\n",
       " 0.002840970464934753,\n",
       " -0.012907605867673108,\n",
       " 0.019173307714834442,\n",
       " 0.0002452607197585296,\n",
       " -0.011072001633923747,\n",
       " -0.015048062719630242,\n",
       " 0.004014978728545309,\n",
       " 0.0027339475292046367,\n",
       " -0.0029058328925585775,\n",
       " -0.010877413885390977,\n",
       " 0.01586532809697106,\n",
       " 0.0018420904299456151,\n",
       " 0.011610359177841804,\n",
       " 0.009106672544926734,\n",
       " -0.018394958583348546,\n",
       " -0.01142874382170154,\n",
       " 0.0334689660877638,\n",
       " -0.010488240250970622,\n",
       " 0.01074768996146592,\n",
       " -0.0006056521204924911,\n",
       " -0.0015161572084387259,\n",
       " 0.017188519571587374,\n",
       " 0.003940387239457754,\n",
       " 0.02636005407847664,\n",
       " 0.00046376573468836124,\n",
       " -0.035285112198585704,\n",
       " 0.008250489990408399,\n",
       " -0.039514136332929946,\n",
       " 0.030148015008830506,\n",
       " -0.004177134564774222,\n",
       " 0.0077315910350791,\n",
       " 0.0031685252354907052,\n",
       " 0.014412411767107095,\n",
       " -0.01061796510621827,\n",
       " -0.0034571625960384926,\n",
       " 0.001764255609929285,\n",
       " -0.007225664472142306,\n",
       " 0.02108025964108129,\n",
       " 0.009022351528714152,\n",
       " -0.015696686064545894,\n",
       " -0.025867101304916238,\n",
       " -0.014217824949896919,\n",
       " -0.027164347994747543,\n",
       " 0.011701165924589344,\n",
       " -0.000198742262203092,\n",
       " -0.02744974249002785,\n",
       " -0.02098945382565634,\n",
       " -0.011253616058741417,\n",
       " 0.002935020775441715,\n",
       " -0.014088100094649269,\n",
       " -0.0234282766339637,\n",
       " -0.04358749175856412,\n",
       " -0.01199304615040461,\n",
       " 0.009813672121269958,\n",
       " -0.01015095618612029,\n",
       " 0.010377974449973027,\n",
       " -0.005237633929289058,\n",
       " 0.002550711438922578,\n",
       " -0.02405095519409434,\n",
       " -0.040422209388340895,\n",
       " -0.007978067887520595,\n",
       " -0.02695678785382227,\n",
       " -0.01639719897903157,\n",
       " 0.0013321102854244734,\n",
       " 0.033935975939184376,\n",
       " 0.021651048631641908,\n",
       " 0.01849873958513378,\n",
       " 0.0013621091174551288,\n",
       " 0.0030177203590205587,\n",
       " 0.0008505074238849239,\n",
       " 0.017668500884077863,\n",
       " 0.016384227517961654,\n",
       " -0.007984553618055553,\n",
       " 0.010734717569073415,\n",
       " -0.017460942605797773,\n",
       " 0.022585066471837868,\n",
       " 0.032898177097203184,\n",
       " 0.02030191237224058,\n",
       " 0.003680937761793104,\n",
       " 0.009599626249809727,\n",
       " 0.022066167050847272,\n",
       " 0.018343069013778524,\n",
       " 0.0024761197170043744,\n",
       " -0.012518431301930161,\n",
       " -0.009476388056419626,\n",
       " -0.03634885396270672,\n",
       " -0.006100303145664942,\n",
       " -0.010981193955853613,\n",
       " -0.011539010554021727,\n",
       " 0.008127250865695707,\n",
       " 0.00836724152194095,\n",
       " -0.009567194803167166,\n",
       " 0.03279439795806314,\n",
       " 0.019497618455969676,\n",
       " 0.037360706157257714,\n",
       " 0.009521791895454693,\n",
       " 0.02183266398778217,\n",
       " 0.004650629681068456,\n",
       " 0.028617263393288916,\n",
       " 0.015917217666541084,\n",
       " 0.015982080559826202,\n",
       " 0.016241530270321502,\n",
       " -0.004147946449060437,\n",
       " -0.008062388903733179,\n",
       " -0.009573681465024715,\n",
       " 0.020379746726595614,\n",
       " -0.0036549927441774446,\n",
       " 0.007582407591242692,\n",
       " -0.02023704947895546,\n",
       " 0.009379094647814537,\n",
       " -0.011791973602659475,\n",
       " 0.015670741279760883,\n",
       " -0.0065316377538528845,\n",
       " -0.019497618455969676,\n",
       " -0.01511292468159277,\n",
       " -0.006148949849967486,\n",
       " -0.016903125076307066,\n",
       " -0.014671861477602393,\n",
       " -0.0017512832175367794,\n",
       " -0.021975361235422326,\n",
       " 0.010812551923428448,\n",
       " -0.01102011113303113,\n",
       " -0.010786607138643437,\n",
       " 0.017603639853437925,\n",
       " -0.018797105541484,\n",
       " -0.02615249580019655,\n",
       " 0.0014553487116452215,\n",
       " -0.013264348055450901,\n",
       " 0.02770919220052315,\n",
       " 0.00042282138836886716,\n",
       " 0.014879420687205077,\n",
       " 0.013452449142126123,\n",
       " -0.02591899087448626,\n",
       " -0.020717030791445944,\n",
       " -0.005727344768904572,\n",
       " 0.007290526899766131,\n",
       " -0.003736070662291901,\n",
       " 0.018848995111054024,\n",
       " -0.0028766447768447914,\n",
       " -0.005785720534670847,\n",
       " -0.021560240953571775,\n",
       " 0.008295892898120874,\n",
       " -0.002367475348640519,\n",
       " 0.004991157076847562,\n",
       " -0.01583938331218605,\n",
       " 0.013685954067836409,\n",
       " 0.018472794800348767,\n",
       " -0.005380331176929213,\n",
       " -0.02063919643709091,\n",
       " -0.008522911161973611,\n",
       " -0.02383042359209915,\n",
       " 0.008263462382800905,\n",
       " 0.007251609722588614,\n",
       " 0.004540363880070861,\n",
       " -0.009178021168746811,\n",
       " -0.009606112911667275,\n",
       " -0.003690667056087483,\n",
       " 0.0262951930478367,\n",
       " -0.026853008714682224,\n",
       " 0.017071767108732233,\n",
       " 0.021300793105721662,\n",
       " -0.01595613577504119,\n",
       " -0.011480634322594154,\n",
       " 0.00780942585509543,\n",
       " -0.01530751243012554,\n",
       " 0.02551684391635081,\n",
       " 0.0020820810861908582,\n",
       " 0.017512832175367796,\n",
       " -0.02229967197655756,\n",
       " 0.023700699668174095,\n",
       " 0.01581343852740104,\n",
       " 0.0025085509308162865,\n",
       " -0.02098945382565634,\n",
       " -0.00014158232352955733,\n",
       " -0.0011213077448930162,\n",
       " 0.029784786159195165,\n",
       " -0.007588893787438945,\n",
       " 0.004514418629624554,\n",
       " 0.0006668659172367683,\n",
       " 0.01111740454163622,\n",
       " 0.010455808804328061,\n",
       " -0.0017626340608802218,\n",
       " -0.013322724286878473,\n",
       " -0.022169948052632504,\n",
       " -0.020470554404665747,\n",
       " -0.0024923354403256544,\n",
       " 0.042368081285733036,\n",
       " 0.012142230059902313,\n",
       " -0.019316004962474598,\n",
       " -0.013413531964948607,\n",
       " -0.021884553557352193,\n",
       " -0.01936789453204462,\n",
       " -0.03183443533308217,\n",
       " 0.006015982129452359,\n",
       " 0.0015169679247555954,\n",
       " 0.0018291179211377855,\n",
       " -0.018615492047988923,\n",
       " -0.022481287332697823,\n",
       " 0.0009648272721281623,\n",
       " 0.013750816029798937,\n",
       " 0.01582640998847095,\n",
       " -0.008522911161973611,\n",
       " -0.0068364903720606555,\n",
       " 0.01932897642354451,\n",
       " -0.021352682675291685,\n",
       " 0.009645030088844793,\n",
       " -0.00483873030208238,\n",
       " -0.001220222818962492,\n",
       " -0.043042649415433704,\n",
       " 0.019316004962474598,\n",
       " -0.009301259362136912,\n",
       " -0.015566961209298244,\n",
       " 0.00943098421738456,\n",
       " -0.008419132022833566,\n",
       " -0.035285112198585704,\n",
       " -0.0021907254545547135,\n",
       " 0.014516191837569734,\n",
       " 0.017668500884077863,\n",
       " 0.021910498342137204,\n",
       " -0.008354269129548444,\n",
       " 0.02323368981675352,\n",
       " 0.008438590145761028,\n",
       " -0.010163928578512796,\n",
       " -0.0055619456017468845,\n",
       " -0.025166588390430563,\n",
       " 0.006489477245746593,\n",
       " -0.023441249957678795,\n",
       " -0.002805296153024714,\n",
       " -0.01083849670821346,\n",
       " 0.0017415538068270761,\n",
       " 0.027164347994747543,\n",
       " -0.013984320024186632,\n",
       " 0.019860849168250202,\n",
       " -0.0038722817137358027,\n",
       " -0.020314883833310492,\n",
       " -0.01914736293004943,\n",
       " -0.005571675128871911,\n",
       " 0.026373027402191735,\n",
       " -0.014762668224349932,\n",
       " 0.024154734333234385,\n",
       " 0.00141318820353893,\n",
       " -0.010656882283395787,\n",
       " -0.016604759119956843,\n",
       " 0.008905599065859008,\n",
       " 0.002947993400664869,\n",
       " -0.006959729031112052,\n",
       " 0.016306391300961436,\n",
       " 0.006914325192076985,\n",
       " -0.010423378289008094,\n",
       " 0.0042484831885943,\n",
       " -0.005377087846000438,\n",
       " -0.012641670426642854,\n",
       " 0.003421488516959102,\n",
       " -0.027761081770093173,\n",
       " -0.010202845755690313,\n",
       " 0.0067327107672593145,\n",
       " -0.0071024262787522064,\n",
       " 0.015982080559826202,\n",
       " 0.022455342547912812,\n",
       " -0.030173959793615517,\n",
       " -0.01755174842122272,\n",
       " 0.0021307277904934027,\n",
       " -0.026671393358541958,\n",
       " -0.007796452997041628,\n",
       " -9.511454301988182e-05,\n",
       " 0.013374614787771088,\n",
       " 0.02789080569401823,\n",
       " 0.009197479291674273,\n",
       " -0.0021745099640640814,\n",
       " 0.013984320024186632,\n",
       " 0.0031977133512044913,\n",
       " 0.014036210525079246,\n",
       " -0.005587890386531896,\n",
       " -0.010222304809940368,\n",
       " 0.005140340520683969,\n",
       " -0.0044657719253220094,\n",
       " 0.01711068521723234,\n",
       " 0.002456661128415616,\n",
       " -0.03564834291086623,\n",
       " -0.025387119992425752,\n",
       " 0.016903125076307066,\n",
       " -0.0017918221765940076,\n",
       " -0.016267475055106513,\n",
       " 0.020094352231315303,\n",
       " -0.005775991473207116,\n",
       " -0.008581287393401181,\n",
       " -0.015216704752055407,\n",
       " 0.006311105686196399,\n",
       " -0.008607232178186193,\n",
       " -0.0017383107087289497,\n",
       " 0.011921698457907123,\n",
       " -0.007686187196044034,\n",
       " -4.755727150994091e-05,\n",
       " 0.019757068166464973,\n",
       " -0.02554278870113582,\n",
       " 0.0020318127164239267,\n",
       " -0.00647650438769279,\n",
       " -0.0030615025325912374,\n",
       " -0.00197667981592513,\n",
       " 0.0007240258413583875,\n",
       " -0.018407931907063645,\n",
       " -0.0047641388129948245,\n",
       " 0.015229677144447912,\n",
       " -0.024310404904589637,\n",
       " 0.01639719897903157,\n",
       " 0.004601982976765911,\n",
       " -0.021871580233637094,\n",
       " 0.03948819341079012,\n",
       " -0.010799579531035943,\n",
       " -0.00039363333086274335,\n",
       " -0.02176780109449705,\n",
       " 0.0038106623842101045,\n",
       " -0.010559588874790698,\n",
       " -0.011156322650136328,\n",
       " 0.023026129675828243,\n",
       " -0.0011529281259727347,\n",
       " -0.011266588451133923,\n",
       " -0.019471673671184665,\n",
       " -0.007076481028305898,\n",
       " -0.029525336448699865,\n",
       " -0.002594493612493257,\n",
       " -0.033417076518193777,\n",
       " -0.005581404190335643,\n",
       " -0.014606998584317273,\n",
       " 0.027008677423392292,\n",
       " -0.006849463230114457,\n",
       " -0.003453919497940366,\n",
       " -0.0020431636761826934,\n",
       " -0.01011852473947773,\n",
       " -0.014412411767107095,\n",
       " -0.0078029396588991775,\n",
       " -0.0084969663771886,\n",
       " 0.0185376558309887,\n",
       " 0.012544376086715173,\n",
       " -0.004806299321101116,\n",
       " -0.004686303992978495,\n",
       " -0.004024708255670337,\n",
       " -0.028980494105569445,\n",
       " -0.0055327574860330984,\n",
       " 0.01827820798313859,\n",
       " -0.0014861583764080705,\n",
       " 0.024089873302594447,\n",
       " 0.20216294023656486,\n",
       " 6.734534820764608e-05,\n",
       " -0.003056637769028724,\n",
       " 0.04179729229517242,\n",
       " 0.0071024262787522064,\n",
       " 0.003296628425273967,\n",
       " 0.015891272881756072,\n",
       " 0.02122295688872144,\n",
       " -0.021002425286726255,\n",
       " 0.019770041490180072,\n",
       " -0.019912738737820224,\n",
       " 0.013426504357341112,\n",
       " -0.02119701210393643,\n",
       " 0.0011991424484940221,\n",
       " 0.014970228365275208,\n",
       " -0.005775991473207116,\n",
       " -0.03131553591209157,\n",
       " -0.008120765135160751,\n",
       " -0.03523322262901568,\n",
       " -0.012894633475280602,\n",
       " -0.012148715790437269,\n",
       " -0.0026447619822601884,\n",
       " -0.017538776960152807,\n",
       " -0.0020480284397452067,\n",
       " 0.03611534903699643,\n",
       " 0.013906485669831598,\n",
       " 0.0030452868092699574,\n",
       " 0.006810545587275644,\n",
       " 0.018018758272643293,\n",
       " 0.00690135279968448,\n",
       " -0.012155202452294818,\n",
       " -0.03461054406888504,\n",
       " 0.013893513277439092,\n",
       " 0.012790853404817965,\n",
       " -0.001002123133087264,\n",
       " -0.007978067887520595,\n",
       " -0.0077056457846327924,\n",
       " 0.0017545263156349058,\n",
       " 0.017214464356372385,\n",
       " -0.003040422278538092,\n",
       " -0.001924789897109135,\n",
       " 0.01261572471053525,\n",
       " 0.015476154462550705,\n",
       " -0.015216704752055407,\n",
       " -0.0167344830438819,\n",
       " 0.0075045727712263624,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_query(\"hi\")\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/anaconda3/envs/morris_env/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: Pinecone\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "#Decide whether to use Pinecone or Chroma.\n",
    "\n",
    "if(Localize):\n",
    "    persist_directory = 'db2'\n",
    "    vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)   \n",
    "    print(\"Database: Chroma\")\n",
    "\n",
    "else:\n",
    "    pc = Pinecone(api_key=Pineconekey)\n",
    "    index = pc.Index(IndexName)\n",
    "    index.describe_index_stats ()\n",
    "\n",
    "    pv= PineconeVectorStore(pinecone_api_key=Pineconekey,embedding=embeddings,index_name= IndexName)\n",
    "    vectorstore = pv.from_documents(texts, embeddings,index_name = IndexName)\n",
    "    print(\"Database: Pinecone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for different robotics tasks. In [40], the author introduced\n",
      "robotGPT, by presenting ChatGPT’s principles and discussing\n",
      "how to enhance robotic intelligence through the medium of\n",
      "ChatGPT.\n",
      "Distinct from the approaches presented above, ROS-\n",
      "GPT Vision introduces various innovative elements that sig-\n",
      "nificantly advance the field of robot interaction with visual\n",
      "data. Firstly, ROSGPT Vision divides the robot interaction\n",
      "with visual data into many modules explained in the next\n",
      "section. Second, ROSGPT Vision makes a distinction between\n",
      "the extraction of image semantics and the robot ontology\n",
      "associated with one given task. Third, it completely separates\n",
      "the prompting logic from the whole architecture, so that the\n",
      "end user will focus mostly on developing the most accurate\n",
      "prompts for his task in the YAML files associated with it.\n",
      "Finally, the shared ROS package is designed following a clear\n",
      "and extensible architecture that could be used and understood\n"
     ]
    }
   ],
   "source": [
    "query = \"What is ros\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\n",
    "from langchain.prompts import PromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\",api_key= OpenAIKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"You are an expert researcher. Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say you don't know. DO NOT try to make up an answer.\n",
    "If the question is not related to the context or chat history, politely respond that you are tuned to only answer questions that are related to the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful answer in markdown:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ROSGPT is a system that includes ROSGPT and ROSGPT Vision. ROSGPT is a broker that converts human commands given in natural language to explicit robotic commands. On the other hand, ROSGPT Vision is a broker that converts image data into natural human language to benefit from large language models (LLMs) and convert it into more customizable robot commands.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 885, 'total_tokens': 960}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"ROSGPT\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ROSGPT is a system that includes ROSGPT and ROSGPT Vision. ROSGPT is a broker that converts human commands given in natural language to explicit robotic commands. On the other hand, ROSGPT Vision is a broker that converts image data into natural human language to benefit from the power of Large Language Models (LLMs) and convert it into more customizable robot commands.', response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 885, 'total_tokens': 963}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"whst is ROS\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"ROSGPT\"),\n",
    "            AIMessage(content=\"ROSGPT is a framework that includes ROSGPT and ROSGPT Vision. ROSGPT is a broker that converts human commands given in natural language to explicit robotic commands. On the other hand, ROSGPT Vision allows the execution of robotic tasks using visual and language prompts, enabling robots to address complex real-world scenarios by processing visual data and making informed decisions.\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg =conversational_qa_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but the provided context and information do not directly relate to common ways of Task Decomposition. If you have any other questions related to the context provided, feel free to ask.\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 789, 'total_tokens': 828}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_question = \"What are common ways of doing it?\"\n",
    "conversational_qa_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
